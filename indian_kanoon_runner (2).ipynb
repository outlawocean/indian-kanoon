{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Indian Kanoon API Runner\n",
        "Given a list of plant names (expects a newline delimited `plant-names.txt`), runs queries against the API for every plant name. Queries are automatically authenticated using the token (redacted, will need to provide one at `XXXXXX`) seen below and requested for dates since Jan 1st 2019.\n"
      ],
      "metadata": {
        "id": "nMLaxVSRHESe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import json\n",
        "\n",
        "# Constants\n",
        "API_URL = \"https://api.indiankanoon.org/search/\"\n",
        "HEADERS = {\"Content-Type\": \"application/json\", \"Authorization\" : \"Token XXXXXX\"}\n",
        "PARAMS_TEMPLATE = \"?formInput={company_name}&pagenum=1&fromDate=01-01-2019\"\n",
        "MAX_QUERIES = 100\n",
        "\n",
        "num_queries = 0\n",
        "# Load company names from a text file into a set to ensure uniqueness\n",
        "def load_company_names(file_path):\n",
        "    with open(file_path, \"r\") as f:\n",
        "        company_names = {line.strip() for line in f if line.strip()}  # Use a set for uniqueness\n",
        "    return list(company_names)  # Convert back to a list for processing\n",
        "\n",
        "# Function to query the API\n",
        "def query_api(company_name):\n",
        "    global num_queries\n",
        "    if num_queries > MAX_QUERIES:\n",
        "        print(\"Maximum number of queries reached. Exiting.\")\n",
        "        return {\"company\": company_name, \"data\": None}\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            API_URL + PARAMS_TEMPLATE.format(company_name=company_name),\n",
        "            headers=HEADERS\n",
        "        )\n",
        "        num_queries += 1\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        print(f\"Query for {company_name} successful.\")\n",
        "        return {\"company\": company_name, \"data\": data}\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Query for {company_name} failed: {e}\")\n",
        "        return {\"company\": company_name, \"data\": None}\n",
        "\n",
        "def process_results(results, output_file):\n",
        "    processed_data = []\n",
        "    for result in results:\n",
        "        if result[\"data\"]:\n",
        "            for doc in result[\"data\"].get(\"docs\", []):\n",
        "                processed_data.append({\n",
        "                    \"company\": result[\"company\"],\n",
        "                    \"tid\": doc.get(\"tid\"),\n",
        "                    \"title\": doc.get(\"title\"),\n",
        "                    \"publishdate\": doc.get(\"publishdate\"),\n",
        "                    \"docsource\": doc.get(\"docsource\"),\n",
        "                    \"headline\": doc.get(\"headline\"),\n",
        "                })\n",
        "\n",
        "    if not processed_data:\n",
        "        print(\"No data to save.\")\n",
        "        return\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    new_data_df = pd.DataFrame(processed_data)\n",
        "\n",
        "    # Rename columns to human-readable names\n",
        "    new_data_df = new_data_df.rename(columns={\n",
        "        \"company\": \"Company\",\n",
        "        \"tid\": \"Document URL\",\n",
        "        \"title\": \"Title\",\n",
        "        \"publishdate\": \"Publication Date\",\n",
        "        \"docsource\": \"Source\",\n",
        "        \"headline\": \"Headline\",\n",
        "    })\n",
        "\n",
        "    # Transform 'Document URL' to a clickable hyperlink\n",
        "    new_data_df[\"Document URL\"] = new_data_df[\"Document URL\"].apply(\n",
        "        lambda x: f\"https://indiankanoon.org/doc/{x}/\" if pd.notnull(x) else x\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # If the output file exists, load it and append new data\n",
        "        existing_df = pd.read_csv(output_file)\n",
        "\n",
        "        # Prevent duplicate entries based on unique identifiers (e.g., \"Document URL\")\n",
        "        combined_df = pd.concat([existing_df, new_data_df], ignore_index=True)\n",
        "        combined_df.drop_duplicates(subset=[\"Document URL\"], inplace=True)\n",
        "    except FileNotFoundError:\n",
        "        # If the file does not exist, create a new one\n",
        "        combined_df = new_data_df\n",
        "\n",
        "    # Save the combined DataFrame back to the file\n",
        "    combined_df.to_csv(output_file, index=False)\n",
        "    print(f\"Results aggregated and saved to {output_file}.\")\n",
        "\n",
        "\n",
        "# Multithreaded execution\n",
        "def multithreaded_queries(company_names, output_file, max_workers=5):\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        results = list(executor.map(query_api, company_names))\n",
        "    process_results(results, output_file)\n",
        "\n",
        "# Load company names from file\n",
        "company_names_file = \"plant-names.txt\"  # Replace with your file path\n",
        "company_names = load_company_names(company_names_file)\n",
        "\n",
        "# Output file\n",
        "output_csv = \"api_results.csv\"\n",
        "\n",
        "# Perform multithreaded queries\n",
        "print(\"Starting multithreaded queries...\")\n",
        "multithreaded_queries(company_names, output_csv, max_workers=10)\n",
        "print(\"All queries completed.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0QjtWunTAIq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TtQMcqJmIat3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Subsequent CSV processing\n",
        "- Minor cleaning to the `headline` field\n",
        "- Convert the `Document URL` field to a hyperlink\n",
        "- Export as excel sheet"
      ],
      "metadata": {
        "id": "gsX92e8nQhNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Required to keep excel happy\n",
        "def clean_dataframe(df):\n",
        "    # Define a function to clean illegal characters in each cell\n",
        "    def clean_illegal_chars(value):\n",
        "        if isinstance(value, str):\n",
        "            # Remove all control characters using a regex\n",
        "            return re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]', '', value)\n",
        "        return value\n",
        "\n",
        "    # Apply the cleaning function to the entire DataFrame\n",
        "    return df.apply(lambda col: col.map(clean_illegal_chars))\n",
        "\n",
        "def process_csv(input_file, output_file):\n",
        "    # Load the CSV into a DataFrame\n",
        "    try:\n",
        "        df = pd.read_csv(input_file)\n",
        "        print(f\"Loaded {input_file} with {len(df)} rows.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File {input_file} not found!\")\n",
        "        return\n",
        "\n",
        "    # Remove <b> tags in Headline\n",
        "    if 'Headline' in df.columns:\n",
        "        df['Headline'] = df['Headline'].apply(\n",
        "            lambda x: x.replace('<b>', '').replace('</b>', '') if pd.notnull(x) else x\n",
        "        )\n",
        "        print(\"Updated `Headline` field.\")\n",
        "\n",
        "    # Remove <b> tags in Title\n",
        "    if 'Title' in df.columns:\n",
        "        df['Title'] = df['Title'].apply(\n",
        "            lambda x: x.replace('<b>', '').replace('</b>', '') if pd.notnull(x) else x\n",
        "        )\n",
        "        print(\"Updated `Title` field.\")\n",
        "\n",
        "    # Convert the `Document URL` field into clickable hyperlinks\n",
        "    if 'Document URL' in df.columns:\n",
        "        df['Document URL'] = df['Document URL'].apply(lambda url: f'=HYPERLINK(\"{url}\", \"link\")' if url else \"\")\n",
        "        print(\"Converted `Document URL` to hyperlinks.\")\n",
        "\n",
        "    # Save the updated DataFrame to a new CSV file\n",
        "    df.to_csv(output_file, index=False, escapechar='\\\\')\n",
        "    print(f\"Updated CSV saved to {output_file}.\")\n",
        "\n",
        "    # Save to excel (so hyperlinks are recognizable)\n",
        "    # Clean the DataFrame first so it plays nicely with excel\n",
        "    df = clean_dataframe(df)\n",
        "    df.to_excel('api_results.xlsx', index=False)\n",
        "\n",
        "input_csv = \"api_results.csv\"\n",
        "output_csv = \"api_results_updated.csv\"\n",
        "\n",
        "# Process the CSV file\n",
        "process_csv(input_csv, output_csv)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPLbh3JdQr-l",
        "outputId": "1f9b4334-ba77-4817-d197-621e8f690f8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded api_results.csv with 244 rows.\n",
            "Updated `Headline` field.\n",
            "Updated `Title` field.\n",
            "Converted `Document URL` to hyperlinks.\n",
            "Updated CSV saved to api_results_updated.csv.\n"
          ]
        }
      ]
    }
  ]
}